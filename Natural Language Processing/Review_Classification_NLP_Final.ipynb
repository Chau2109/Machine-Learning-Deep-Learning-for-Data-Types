{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Text Classification Lab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "71s3i8d3EPrx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qg4pHgh3OSqn"
   },
   "source": [
    "### Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>Now, I won't deny that when I purchased this o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>The saddest thing about this \"tribute\" is that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>Last night I decided to watch the prequel or s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>I have to admit that i liked the first half of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>I was not impressed about this film especially...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>pos</td>\n",
       "      <td>This film is fun, if your a person who likes a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>pos</td>\n",
       "      <td>After seeing this film I feel like I know just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>neg</td>\n",
       "      <td>first this deserves about 5 stars due to actin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>neg</td>\n",
       "      <td>If you like films that ramble with little plot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>neg</td>\n",
       "      <td>As interesting as a sheet of cardboard, this d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                               text\n",
       "0       neg  Now, I won't deny that when I purchased this o...\n",
       "1       neg  The saddest thing about this \"tribute\" is that...\n",
       "2       neg  Last night I decided to watch the prequel or s...\n",
       "3       neg  I have to admit that i liked the first half of...\n",
       "4       neg  I was not impressed about this film especially...\n",
       "...     ...                                                ...\n",
       "24995   pos  This film is fun, if your a person who likes a...\n",
       "24996   pos  After seeing this film I feel like I know just...\n",
       "24997   neg  first this deserves about 5 stars due to actin...\n",
       "24998   neg  If you like films that ramble with little plot...\n",
       "24999   neg  As interesting as a sheet of cardboard, this d...\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.read_csv(\"dataset/reviews.csv\")\n",
    "dataset=dataset[['sentiment', 'text']]\n",
    "dataset = dataset.rename(columns={'sentiment': 'class'})\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Split the dataset into two data structures (pandas frames), one for the reviews (our documents) (X) and one for the class (y).\n",
    "Check that the lengths of both dataframes are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X is: 25000 , size of y is: 25000\n"
     ]
    }
   ],
   "source": [
    "X = dataset['text']\n",
    "Y = dataset['class']\n",
    "print(\"Size of X is: \" + str(len(X)) + \" , size of y is: \" + str(len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We now begin our preprocessing task, by lowercasing all of the documents, removing special characters and numerical values. Then we tokenize and stem our documents. \n",
    "### Exercise 2\n",
    "Write a function to_lower(X) that takes a dataframe and returns its content in lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(X): \n",
    "    df = pd.DataFrame(X)\n",
    "    df2 = df['text']\n",
    "    df2 = df2.str.lower()\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    now, i won't deny that when i purchased this o...\n",
       "1    the saddest thing about this \"tribute\" is that...\n",
       "2    last night i decided to watch the prequel or s...\n",
       "3    i have to admit that i liked the first half of...\n",
       "4    i was not impressed about this film especially...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=to_lower(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 \n",
    "Write a function clean_text(X), that takes the dataset as an input and removes all special characters and numerical values from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(X):\n",
    "    df = pd.DataFrame(X)\n",
    "    df2 = df['text'].apply(lambda x: re.sub(\"[^a-z\\s]\", \"\", x))\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=clean_text(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        now i wont deny that when i purchased this off...\n",
       "1        the saddest thing about this tribute is that a...\n",
       "2        last night i decided to watch the prequel or s...\n",
       "3        i have to admit that i liked the first half of...\n",
       "4        i was not impressed about this film especially...\n",
       "                               ...                        \n",
       "24995    this film is fun if your a person who likes a ...\n",
       "24996    after seeing this film i feel like i know just...\n",
       "24997    first this deserves about  stars due to acting...\n",
       "24998    if you like films that ramble with little plot...\n",
       "24999    as interesting as a sheet of cardboard this di...\n",
       "Name: text, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Write a function tokenize(X) that takes a dataframe and returns the tokens in each row (document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\xps9360\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\xps9360\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\xps9360\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\xps9360\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\xps9360\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\xps9360\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\XPS9360\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize(X):\n",
    "    for i in range(len(X)):\n",
    "        if isinstance(X[i], str):  # Check if the element is already a string\n",
    "            X[i] = word_tokenize(X[i])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [now, i, wont, deny, that, when, i, purchased,...\n",
      "1        [the, saddest, thing, about, this, tribute, is...\n",
      "2        [last, night, i, decided, to, watch, the, preq...\n",
      "3        [i, have, to, admit, that, i, liked, the, firs...\n",
      "4        [i, was, not, impressed, about, this, film, es...\n",
      "                               ...                        \n",
      "24995    [this, film, is, fun, if, your, a, person, who...\n",
      "24996    [after, seeing, this, film, i, feel, like, i, ...\n",
      "24997    [first, this, deserves, about, stars, due, to,...\n",
      "24998    [if, you, like, films, that, ramble, with, lit...\n",
      "24999    [as, interesting, as, a, sheet, of, cardboard,...\n",
      "Name: text, Length: 25000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X = tokenize(X)\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Write a function remove_stop_words(X, stop_words), that takes a dataset X and the set of stop words you want removed from it. Your function should return the dataset, free of any common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\XPS9360\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "def remove_stop_words(X, stop_words):\n",
    "    X = pd.Series(X)\n",
    "    X = X.apply(lambda words_list: [word for word in words_list if word not in stop_words])\n",
    "    return X\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [wont, deny, purchased, ebay, high, expectatio...\n",
       "1        [saddest, thing, tribute, almost, singers, inc...\n",
       "2        [last, night, decided, watch, prequel, shall, ...\n",
       "3        [admit, liked, first, half, sleepers, looked, ...\n",
       "4        [impressed, film, especially, fact, went, cine...\n",
       "                               ...                        \n",
       "24995    [film, fun, person, likes, good, campy, featur...\n",
       "24996    [seeing, film, feel, like, know, little, bit, ...\n",
       "24997    [first, deserves, stars, due, acting, would, g...\n",
       "24998    [like, films, ramble, little, plot, exposition...\n",
       "24999    [interesting, sheet, cardboard, dispensable, p...\n",
       "Name: text, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = remove_stop_words(X, stopwords.words('english'))\n",
    "X\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "Write a function stem(X), that takes a dataframe X and returns the stems of all the words in it.\n",
    "\n",
    "You're free to choose any stemmer you want.\n",
    "\n",
    "It's also possible to use a lemmatizer (lemmatization will be a lot slower!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "def stem(X):\n",
    "    stemmer = PorterStemmer()\n",
    "    X = X.apply(lambda words_list: [stemmer.stem(word) for word in words_list])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stem(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [wont, deni, purchas, ebay, high, expect, incr...\n",
       "1        [saddest, thing, tribut, almost, singer, inclu...\n",
       "2        [last, night, decid, watch, prequel, shall, sa...\n",
       "3        [admit, like, first, half, sleeper, look, good...\n",
       "4        [impress, film, especi, fact, went, cinema, fa...\n",
       "                               ...                        \n",
       "24995    [film, fun, person, like, good, campi, featur,...\n",
       "24996    [see, film, feel, like, know, littl, bit, usa,...\n",
       "24997    [first, deserv, star, due, act, would, give, b...\n",
       "24998    [like, film, rambl, littl, plot, exposit, spic...\n",
       "24999    [interest, sheet, cardboard, dispens, period, ...\n",
       "Name: text, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "Having called a tokenizer and a stemmer on our dataset, the resulting rows are now of type list.\n",
    "We need to convert them back to str, as our CountVectorizer expects a dataset where every document is a string. \n",
    "Define a function to_String(X), that takes your dataset and stitches back its rows back to the str format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_String(X):\n",
    "    X = X.apply(lambda x: ' '.join(x))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = to_String(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        wont deni purchas ebay high expect incred outo...\n",
       "1        saddest thing tribut almost singer includ othe...\n",
       "2        last night decid watch prequel shall say call ...\n",
       "3        admit like first half sleeper look good act ev...\n",
       "4        impress film especi fact went cinema famili go...\n",
       "                               ...                        \n",
       "24995    film fun person like good campi featur film ev...\n",
       "24996    see film feel like know littl bit usa david ly...\n",
       "24997    first deserv star due act would give better su...\n",
       "24998    like film rambl littl plot exposit spice kinki...\n",
       "24999    interest sheet cardboard dispens period piec l...\n",
       "Name: text, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = X #a is used for exercise 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Space Model\n",
    "Now that we preprocessed our corpus, we can proceed to vectorize it.\n",
    "### Exercise 8\n",
    "We can now use the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from sklearn to create our document-term-matrix.\n",
    "\n",
    "a. Create a document-term matrix from your dataset X, use min_df and max_df parameters to exclude words that appear in less than 10 documents, and words that appear in more than 99.5% of the documents. We want to keep only words of medium frequency, as stated in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_document_term_matrix(X):\n",
    "    vectorizer = CountVectorizer(min_df=10, max_df=0.995) \n",
    "    dtm = vectorizer.fit_transform(X)\n",
    "    pd.DataFrame(vectorizer.transform(X).toarray(), columns= vectorizer.get_feature_names_out())\n",
    "    print(dtm)\n",
    "    return vectorizer, dtm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 13623)\t1\n",
      "  (0, 3078)\t1\n",
      "  (0, 9608)\t1\n",
      "  (0, 3696)\t2\n",
      "  (0, 5675)\t1\n",
      "  (0, 4134)\t1\n",
      "  (0, 6100)\t1\n",
      "  (0, 13638)\t1\n",
      "  (0, 7504)\t1\n",
      "  (0, 2326)\t1\n",
      "  (0, 3902)\t2\n",
      "  (0, 5868)\t1\n",
      "  (0, 11318)\t1\n",
      "  (0, 3287)\t1\n",
      "  (0, 521)\t1\n",
      "  (0, 4668)\t1\n",
      "  (0, 243)\t3\n",
      "  (0, 3236)\t1\n",
      "  (0, 13347)\t1\n",
      "  (0, 5145)\t2\n",
      "  (0, 11189)\t1\n",
      "  (0, 11896)\t1\n",
      "  (0, 7364)\t1\n",
      "  (0, 4805)\t1\n",
      "  (0, 1688)\t1\n",
      "  :\t:\n",
      "  (24999, 1947)\t1\n",
      "  (24999, 4358)\t1\n",
      "  (24999, 7097)\t1\n",
      "  (24999, 4226)\t1\n",
      "  (24999, 4070)\t1\n",
      "  (24999, 5072)\t1\n",
      "  (24999, 9110)\t1\n",
      "  (24999, 5408)\t1\n",
      "  (24999, 9011)\t1\n",
      "  (24999, 4728)\t1\n",
      "  (24999, 8087)\t1\n",
      "  (24999, 12163)\t1\n",
      "  (24999, 10866)\t1\n",
      "  (24999, 9812)\t1\n",
      "  (24999, 3971)\t1\n",
      "  (24999, 4327)\t1\n",
      "  (24999, 10131)\t1\n",
      "  (24999, 8732)\t1\n",
      "  (24999, 3424)\t1\n",
      "  (24999, 11402)\t1\n",
      "  (24999, 1752)\t1\n",
      "  (24999, 6587)\t1\n",
      "  (24999, 13634)\t1\n",
      "  (24999, 10273)\t1\n",
      "  (24999, 3357)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer, X = create_document_term_matrix(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Print the size and the contents of your vocab (feature space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 13823\n",
      "Vocabulary Contents:\n",
      "First 10 Words in Vocabulary:\n",
      "['aag' 'aam' 'aaron' 'ab' 'abandon' 'abba' 'abbey' 'abbi' 'abbot' 'abbott']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(\"Vocabulary Size:\", vocabulary_size)\n",
    "print(\"Vocabulary Contents:\")\n",
    "print(\"First 10 Words in Vocabulary:\")\n",
    "print(vocabulary[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our Logistic Regressor\n",
    "\n",
    "### Exercise 9:\n",
    "Before we dive into training our model, let's get our vector of true labels **y** into the right format.\n",
    "Notice that by printing the contents of **y** below, what we get are the labels **neg** and **pos**. \n",
    "The model works only with **1 and 0**.\n",
    "Let's convert the labels accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class\n",
       "0   neg\n",
       "1   neg\n",
       "2   neg\n",
       "3   neg\n",
       "4   neg\n",
       "5   pos\n",
       "6   pos\n",
       "7   neg\n",
       "8   pos\n",
       "9   neg"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.DataFrame(Y)\n",
    "y[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Use the [label_ encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) from Sklearn, to transform the labels in the vector **y** accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XPS9360\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\XPS9360\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "num_rows = len(y)\n",
    "print(num_rows)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "y = label_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a test and train set from our DTM and our vector y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10\n",
    "Use the [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier from Sklearn to train your model, and check its accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XPS9360\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(random_state=100)\n",
    "model.fit(train_X, train_y)\n",
    "train_predictions = model.predict(train_X)\n",
    "test_predictions = model.predict(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9942933333333334\n",
      "Test Accuracy: 0.85888\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = accuracy_score(train_y, train_predictions)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "test_accuracy = accuracy_score(test_y, test_predictions)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our results show a clear sign of overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjwKo9ydE31x"
   },
   "source": [
    "## Regular \n",
    "\n",
    "Our main goal in the regular exercises is to provide a basic implementation for the logistic regression model.\n",
    "\n",
    "\n",
    "We'll first define a function **initialize(X)** to get the initial vector of weights W and bias b.\n",
    "\n",
    "We will then do our **forward_pass(X, W, b)** to get a vector of predictions.\n",
    "\n",
    "We then write a function **gradient_descent(X, W, b, y, lr)** to get the updated vector of weights W and bias b.\n",
    "\n",
    "\n",
    "We conclude this part by writing the model function, which calls all of the functions we defined, and proceed with the learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11\n",
    "Write a function **initialize(X)**, which takes a DTM as an input and returns a vector of weights W and a scalar b for the bias. Both are initialized with some random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=train_X.toarray()\n",
    "train_y=np.array(train_y)\n",
    "train_y=train_y.reshape(train_y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "N89yRfD7aat-"
   },
   "outputs": [],
   "source": [
    "def initialize(X):\n",
    "    num_features = X.shape[1] \n",
    "    # Initialize weights with random values\n",
    "    W = np.random.uniform(-1,1,[num_features,1])  \n",
    "    b = np.random.uniform(-1,1)\n",
    "    return (W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the vector W is: (13823, 1)\n"
     ]
    }
   ],
   "source": [
    "W,b=initialize(train_X)\n",
    "print(\"Shape of the vector W is:\",W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12\n",
    "Write a function **forward_pass(X, W, b)** which takes the DTM X, the vector W and the bias b as inputs and returns a vector of predictions P.\n",
    "\n",
    "Your function should implement the following equations\n",
    "\n",
    "$$Z=X.W + b$$\n",
    "$$P=\\sigma{(Z)}=\\frac{1}{1+e^{-Z}}$$\n",
    "\n",
    "You can also implement the sigmoid as a separate function, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "qg9ZFBlYfHGQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "def forward_pass(X, W, b):\n",
    "    if issparse(X):\n",
    "        X = X.toarray()\n",
    "    Z = np.dot(X, W) + b\n",
    "    P = 1 / (1 + np.exp(-Z))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector of predictions shape is: (18750, 1)\n"
     ]
    }
   ],
   "source": [
    "P=forward_pass(train_X, W, b)\n",
    "print(\"The vector of predictions shape is:\",P.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyR7cN5gFl5O"
   },
   "source": [
    "### Exercise 13\n",
    "Calculating the loss/cost function\n",
    "You can use the [implementation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html) by Sklearn, but feel free to also implement your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "36N1AVywgUy6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 0.46933045662909223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# predict the probabilities of the positive class\n",
    "y_prob = model.predict_proba(test_X)[:, 1]\n",
    "loss = log_loss(test_y, y_prob)\n",
    "\n",
    "print(\"Log Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vonvu9_UGM6x"
   },
   "source": [
    "### Exercise 14\n",
    "Write a function **gradient_descent(X, W, b , P, y, lr)** and returns the updated weight vector W, and bias b.\n",
    "Your function needs to implement the following equations:\n",
    "\n",
    "$$dW=\\frac{1}{ne}X^T . (P-y)$$\n",
    "\n",
    "$$db=\\frac{1}{ne} \\sum(P-y)$$\n",
    "\n",
    "$$W=W-\\alpha dW$$\n",
    "\n",
    "$$W=b-\\alpha db$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "GWL981CJjPbs"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, W, b, P, y, lr):\n",
    "    # Number of samples\n",
    "    n = len(y)\n",
    "    dW = (1 / n) * np.dot(X.T, (P - y))\n",
    "    db = (1 / n) * np.sum(P - y)\n",
    "    W = W - lr * dW\n",
    "    b = b - lr * db\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the updated W is: (13823, 1)\n"
     ]
    }
   ],
   "source": [
    "W, b = gradient_descent(train_X, W, b, P, train_y, lr=0.2)\n",
    "print(\"shape of the updated W is:\", W.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZIYHzIyIASN"
   },
   "source": [
    "### Exercise 15\n",
    "Now we can implement our logistic regression model in 3 simple steps\n",
    "1. initialize the vector W and the bias b\n",
    "\n",
    "2. repeat until number of iterations is reached   \n",
    "    2.1. get a vector of predictions P  \n",
    "    2.2. update the weights W and bias b using gradient descent  \n",
    "    \n",
    "3. return the final vector W and bias b\n",
    "\n",
    "**logistic_regression(X, y, lr, iters)**, takes the matrix X as an input, the vector of true labels y, a learning rate, and the number of iterations iters. The function returns the learned parameters of the model, namely W and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "Clu3dOgHmS64"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def logistic_regression(X, y, lr, iters):\n",
    "    W,b = initialize(X)\n",
    "    print(W)\n",
    "    for i in tqdm(range(iters)):\n",
    "        P = forward_pass(X, W, b)\n",
    "        W, b = gradient_descent(train_X, W, b, P, y, lr)\n",
    "        \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "Vv9snZjqGo7v",
    "outputId": "8923c469-c9a8-414e-978f-491ca7eb42b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.66413928]\n",
      " [-0.5374101 ]\n",
      " [ 0.97096081]\n",
      " ...\n",
      " [-0.69652637]\n",
      " [ 0.4793068 ]\n",
      " [-0.73477369]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [13:21<00:00,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "W, b= logistic_regression(train_X, train_y, lr=1.5, iters=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our model\n",
    "We have been able to implement the model and run it on our training set. It's time to see how well it does. \n",
    "We'll first make a function **predict(X, W, b)**, that takes the dataset and the learned parameters and returns an array of predictions. Our threshold is 0.5, any prediction below that is returned as 0, and any above it are returned as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, b):\n",
    "    P = forward_pass(X, W, b)\n",
    "    P=1*(P >= 0.5)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_train=predict(train_X, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on our train set is,  87.94133333333333 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on our train set is, \",accuracy_score( train_y, P_train)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_test=predict(test_X, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on our test set is 82.896 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on our test set is\", accuracy_score(test_y, P_test)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "\n",
    "In this part, we set to understand what did the model actually learn.\n",
    "\n",
    "### Exercise 16\n",
    "Using the CountVectorizer of Sklearn, recreate a pandas frame where the rows contain the documents and the columns contain the features. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer expects a list of text document as input, not a DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "def create_document_term_matrix1(X):\n",
    "    vectorizer = CountVectorizer(min_df=10, max_df=0.995) \n",
    "    dtm = vectorizer.fit_transform(a)\n",
    "    dtm_df = pd.DataFrame(vectorizer.transform(a).toarray(), columns= vectorizer.get_feature_names_out())\n",
    "    print(dtm_df)\n",
    "    return vectorizer, dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       aag  aam  aaron  ab  abandon  abba  abbey  abbi  abbot  abbott  ...  \\\n",
      "0        0    0      0   0        0     0      0     0      0       0  ...   \n",
      "1        0    0      0   0        0     0      0     0      0       0  ...   \n",
      "2        0    0      0   0        0     0      0     0      0       0  ...   \n",
      "3        0    0      0   0        0     0      0     0      0       0  ...   \n",
      "4        0    0      0   0        1     0      0     0      0       0  ...   \n",
      "...    ...  ...    ...  ..      ...   ...    ...   ...    ...     ...  ...   \n",
      "24995    0    0      0   0        0     0      0     0      0       0  ...   \n",
      "24996    0    0      0   0        0     0      0     0      0       0  ...   \n",
      "24997    0    0      0   0        0     0      0     0      0       0  ...   \n",
      "24998    0    0      0   0        0     0      0     0      0       0  ...   \n",
      "24999    0    0      0   0        0     0      0     0      0       0  ...   \n",
      "\n",
      "       zombi  zombiesbr  zone  zoo  zoom  zorro  zu  zucker  zulu  zuniga  \n",
      "0          0          0     0    0     0      0   0       0     0       0  \n",
      "1          0          0     0    0     0      0   0       0     0       0  \n",
      "2          0          0     0    0     0      0   0       0     0       0  \n",
      "3          0          0     0    0     0      0   0       0     0       0  \n",
      "4          0          0     0    0     0      0   0       0     0       0  \n",
      "...      ...        ...   ...  ...   ...    ...  ..     ...   ...     ...  \n",
      "24995      0          0     0    0     0      0   0       0     0       0  \n",
      "24996      0          0     0    0     0      0   0       0     0       0  \n",
      "24997      0          0     0    0     0      0   0       0     0       0  \n",
      "24998      0          0     0    0     0      0   0       0     0       0  \n",
      "24999      0          0     0    0     0      0   0       0     0       0  \n",
      "\n",
      "[25000 rows x 13823 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer, dtm_df = create_document_term_matrix1(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18\n",
    "Knowing that our logistic regressor learns a weight for each feature (word in the vocab),  return the words with the highest weights (5 highest), and the words with lowest weights (5 lowest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words with highest weights:\n",
      "mj: 1.422636687043491\n",
      "kurosawa: 1.4536169298882846\n",
      "flawless: 1.616274122921176\n",
      "excel: 1.660043469452013\n",
      "refresh: 1.7238773820786502\n",
      "\n",
      "Bottom 5 words with lowest weights:\n",
      "worst: -2.3043585428276634\n",
      "poorli: -2.1275280847292293\n",
      "wast: -2.030525269106648\n",
      "alright: -1.7525070007953822\n",
      "mstk: -1.6719868518860082\n"
     ]
    }
   ],
   "source": [
    "coefficients = model.coef_[0]\n",
    "word_weight_pairs = [(word, weight) for word, weight in zip(vectorizer.get_feature_names_out(), coefficients)]\n",
    "sorted_word_weight_pairs = sorted(word_weight_pairs, key=lambda x: x[1])\n",
    "top_5_words = sorted_word_weight_pairs[-5:]\n",
    "bottom_5_words = sorted_word_weight_pairs[:5]\n",
    "print(\"Top 5 words with highest weights:\")\n",
    "for word, weight in top_5_words:\n",
    "    print(f\"{word}: {weight}\")\n",
    "\n",
    "print(\"\\nBottom 5 words with lowest weights:\")\n",
    "for word, weight in bottom_5_words:\n",
    "    print(f\"{word}: {weight}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 19\n",
    "Print the weights of the words \"good\" and \"bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of 'good': 0.2337061156360339\n",
      "Weight of 'bad': -0.7694501741865307\n"
     ]
    }
   ],
   "source": [
    "good_weight = None\n",
    "bad_weight = None\n",
    "for word, weight in word_weight_pairs:\n",
    "    if word == \"good\":\n",
    "        good_weight = weight\n",
    "    elif word == \"bad\":\n",
    "        bad_weight = weight\n",
    "\n",
    "print(\"Weight of 'good':\", good_weight)\n",
    "print(\"Weight of 'bad':\", bad_weight)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNfW1OLITzgfZTQ+mqA2JAg",
   "include_colab_link": true,
   "name": "RÃ©gression Logistique",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
